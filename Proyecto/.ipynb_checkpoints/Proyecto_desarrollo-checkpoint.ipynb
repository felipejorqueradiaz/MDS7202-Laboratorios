{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbbdd29",
   "metadata": {
    "cell_id": "00006-84a35c5d-0758-4cbb-b2ca-b182898b80d0",
    "deepnote_cell_height": 295.8833312988281,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "\n",
    "\n",
    "# Proyecto\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- Felipe Jorquera Díaz\n",
    "\n",
    "- Nombre de usuarios en Codalab: felipejorquera\n",
    "\n",
    "- Nombre del Equipo en Codalab: felipejorquera\n",
    "\n",
    "### Link de repositorio de GitHub: `https://github.com/felipejorqueradiaz`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585563b4",
   "metadata": {
    "cell_id": "00007-447f1977-318e-432f-ba83-12d7e667ae68",
    "deepnote_cell_height": 253.13333129882812,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "\n",
    "## 1. Introducción\n",
    "\n",
    "El objetivo de este proyecto consiste en resolver dos problemáticas de negocio asociadas a las aspiraciones de Renacín, el ex-influencer. Ambos problemas se detallan a continuación:\n",
    "* Problema de clasificación: Renacín desea conocer la potencial evaluación de una película por parte de sus espectadores. Para esto se han construido 5 categorías de puntuación: Negative, Mixed, Mostly Positive, Positive y Very Positive.\n",
    "* Problema de regresión: Renacín desea conocer las potenciales ganancias de una película, para ver su rentabilidad.\n",
    "\n",
    "\n",
    "Los datos que proveen es un dataset con 9641 filas y 18 columnas, y luego de un preprocesamiento incial que genera el conjunto de datos que se utilizará en el proyecto finalmente quedan 6451 registros y 13 columnas, dos de las cuales corresponden a las variables objetivo de los dos problemas: la columna numérica \"target\" representa la recaudación de una película y la columna categórica \"label\" muestra la clasificación de la misma. De las 11 variables restantes, 2 de ellas son numéricas, una es temporal y otras 8 son categóricas/texto.\n",
    "\n",
    "El preprocesamiento posterior cumple la función de añadir nuevas variables de interés al problema, lo cual puede mejorar le rendimiento de los modelos. En base a lo anterior es que las variables numéricas son estandarizadas, la variable temporal divida en sus componentes día, mes, año y las variables de texto son codificadas, analizadas y separadas con tal de extraer la mayor información posible.\n",
    "\n",
    "Para el modelo de clasificación es evaluado por la métrica \"F1_macro\" que evalúa tanto la \"precisión\" como el \"recall\" de las predicciones para cada clase. Además, dado que no todas las clases son igual de frecuentes es que el \"F1_macro\" pondera de igual forma cada tipo. Para el modelo de regresión se ocupa \"R2\" que mide el grado de explicabilidad de las variables dependientes sobre la variable independiente.\n",
    "\n",
    "Para el desarrollo de los modelos, se utilizó un baseline para cada uno con un modelo Dummy y que representa el azar, por lo cual se esperaba que los modelos posteriores pudieran sobrepasar a una asignación aleatoria. Para clasificación se ocuparon Árboles de Decisión, Regresión Logística, Random Forest, KNeighbors y XGBoost, siendo este último el que mayor desempeño mostró en el conjunto de testeo. Para la regresión se ocuparon Regresiones Lineales, Random Forest y XGboost, siendo la regresión lineal la escogida dado que poseía un performance similar al modelo con mejor rendimiento (XGBoost) pero dado que la regresión linal si otorga explicabilidad a las variables y su tiempo de entrenamiento era menor es que se prefirió por este método.\n",
    "\n",
    "Los resultados de los modelos es bueno dado que superan al baseline desarrollado en este mismo proyecto, pero además superan el baseline impuesto en la competencia, por lo que tuvieron un gran rendimiento. Sin embargo, dada la gran cantidad de variables numéricas es que es posible que los modelos puedan tener un mejor desempeño si se procesa de mejor forma el texto, lo cual por razones de capacidad computacional y tiempo no se pudo realizar.\n",
    "\n",
    "En métodos generales, el preprocesamiento de variables logró que ambos modelos desarrollados tengan una gran performance a la hora de predecir métricas de interés sobre las películas por lo cual se cumplieron con los objetivos del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7922658",
   "metadata": {
    "cell_id": "00008-6607fdcd-a35f-4e75-9b46-da3b181c1551",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "## 2. Preparación y Análisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064c2a3",
   "metadata": {},
   "source": [
    "Por razones de conveniencia de lectura, se ha decidido dejar el Análisis Exploratorio en un documento aparte dada la cantidad de gráficas y código existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e25fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "#pip install pyarrow\n",
    "#pip install seaborn\n",
    "#pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db784824",
   "metadata": {},
   "source": [
    "Preprocesamos según lo indicado en el instructivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5821d013",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.pickle', \"rb\") as fh:\n",
    "    data_val = pickle.load(fh)\n",
    "data_val['tipo'] = 'val'\n",
    "\n",
    "data1 = pd.read_parquet('train_numerical_features.parquet')\n",
    "data2 = pd.read_parquet('train_text_features.parquet')  \n",
    "                           \n",
    "data = data1.merge(data2, on = ['id', 'tagline', 'credits', 'title'])\n",
    "data['tipo'] = 'traintest'\n",
    "                           \n",
    "data = pd.concat([data, data_val], axis=0)\n",
    "                           \n",
    "data.drop(['poster_path', 'backdrop_path', 'recommendations'], axis=1, inplace=True)\n",
    "\n",
    "data = data[~ (\n",
    "    (data['revenue']==0) |\n",
    "    (data['release_date'].isna()) |\n",
    "    (data['runtime'].isna()) |\n",
    "    ((data['status'] != 'Released'))\n",
    "                )]\n",
    "\n",
    "data['release_date'] = pd.to_datetime(data['release_date'], format = '%Y-%m-%d')\n",
    "\n",
    "data_cat = data.select_dtypes(include=[object])\n",
    "data[data_cat.columns] = data_cat.fillna('')\n",
    "\n",
    "data.loc[(data['vote_average'] <= 10) & (data['vote_average'] > 8), 'label'] = 'Very Positive'\n",
    "data.loc[(data['vote_average'] <= 8) & (data['vote_average'] > 7), 'label'] = 'Positive'\n",
    "data.loc[(data['vote_average'] <= 7) & (data['vote_average'] > 6), 'label'] = 'Mostly Positive'\n",
    "data.loc[(data['vote_average'] <= 6) & (data['vote_average'] > 5), 'label'] = 'Mixed'\n",
    "data.loc[(data['vote_average'] <= 5) , 'label'] = 'Negative'\n",
    "\n",
    "data.drop(['vote_average', 'id', 'status'], axis=1, inplace=True)\n",
    "data.rename(columns = {'revenue':'target'},\n",
    "            inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7780b9",
   "metadata": {
    "cell_id": "00011-95957584-71f1-4669-a6e5-9b8ac7b8d4e0",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Preprocesamiento, Holdout y Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "021e21de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize  \n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f499def",
   "metadata": {},
   "source": [
    "Añadimos variables temporales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea4b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['day'] = data['release_date'].dt.day\n",
    "data['month'] = data['release_date'].dt.month\n",
    "data['year'] = data['release_date'].dt.year\n",
    "data['day_name'] = data['release_date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc0174",
   "metadata": {},
   "source": [
    "Codificamos solamente el idioma \"inglés\" dado que es el predominante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145ad544",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['en_language'] = np.where(data['original_language'] == 'en', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45005b68",
   "metadata": {},
   "source": [
    "Transformamos la variable budget a logaritmica dada su distribución. Cambiamos los valores indeterminados por cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf24dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "data['log_budget'] = np.log(data['budget'])\n",
    "data['log_budget'].replace(np.inf, 0, inplace=True)\n",
    "data['log_budget'].replace(-np.inf, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852c5d7",
   "metadata": {},
   "source": [
    "Para aquellas columnas que muestran más de una \"categoría\", se agrega la cantidad de valores encontrados. Por ejemplo, cuántos \"genres\" posee una película."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db602394",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['genres', 'credits', 'production_companies', 'keywords']:\n",
    "    values = [x.split(sep='-') for x in data[col]]\n",
    "    len_values = [len(x) for x in values]\n",
    "    data[f'len_{col}'] = len_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7170a",
   "metadata": {},
   "source": [
    "Para la variable \"genres\", se toman los 10 géneros más frecuentes y se crea una variable nueva para cada uno: 1 si posee el género, 0 si no.\n",
    "Además, se genera una variable que resume las anteriores e indica si posee un género popular o no. También se crea la variable \"Other\" en caso de que la pelicula posea un género no popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8c212a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_genres = ['Drama', 'Action', 'Comedy', 'Adventure', 'Thriller',\n",
    "                 'Fantasy', 'Science Fiction', 'Family', 'Crime', 'Romance']\n",
    "for pop in popular_genres:\n",
    "    data[f'genre_{pop}'] = pd.DataFrame(np.where(data['genres'].str.contains(pop), 1, 0))\n",
    "    \n",
    "data['genre_Other'] = data.loc[:,data.columns[-len(popular_genres):]].sum(1)\n",
    "data['genre_Popular'] = np.where(data['genre_Other']>0, 1, 0)\n",
    "data['genre_Other'] = np.where(data['genre_Other']==0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfde35d",
   "metadata": {},
   "source": [
    "Creamos una variable binaria que indica si la pelicula pertenece a una de las 50 productoras más famosas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8dd3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = [x.split(sep='-') for x in data['production_companies']]\n",
    "values = [i for x in companies for i in x]\n",
    "val_unique, counts = np.unique(values, return_counts=True)\n",
    "series_count = pd.Series(counts, index = val_unique).sort_values(ascending = False).head(50)\n",
    "\n",
    "counter2 = np.zeros(len(data))\n",
    "for pop in series_count.index:\n",
    "    counter2 = counter2 + np.where(data['production_companies'].str.contains(pop), 1, 0)\n",
    "data['Popular_company'] = counter2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f997af",
   "metadata": {},
   "source": [
    "Creamos una variable binaria que indica si la pelicula posee a uno de los 150 actores o actrices más famosas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953bb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actores = [x.split(sep='-') for x in data['credits']]\n",
    "values = [i for x in actores for i in x]\n",
    "val_unique, counts = np.unique(values, return_counts=True)\n",
    "series_count = pd.Series(counts, index = val_unique).sort_values(ascending = False).head(150)\n",
    "\n",
    "counter2 = np.zeros(len(data))\n",
    "for pop in series_count.index:\n",
    "    counter2 = counter2 + np.where(data['credits'].str.contains(pop), 1, 0)\n",
    "data['Popular_actor'] = counter2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56ed82",
   "metadata": {},
   "source": [
    "Ahora creamos el Tokenizer pera procesar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bffceeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seteamos el idioma en español\n",
    "stop_words = stopwords.words('spanish')\n",
    "\n",
    "class StemmerTokenizer:\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        doc_tok = word_tokenize(doc)\n",
    "        doc_tok = [t for t in doc_tok if t not in stop_words]\n",
    "        return [self.ps.stem(t) for t in doc_tok]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b73fc",
   "metadata": {
    "cell_id": "ec9c0f8a2b044f858858ef3c3248c239",
    "deepnote_cell_height": 102,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "source": [
    "Creamos la Pipeline con los preprocesamientos para columnas numéricas y texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7f1cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_num = ['budget', 'runtime', 'log_budget']\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('StandardScaler', StandardScaler(), col_num),\n",
    "        ('OneHotEncoder', OneHotEncoder(), ['day_name']),\n",
    "        ('BagOfWords1', CountVectorizer(tokenizer= StemmerTokenizer(),\n",
    "                                               ngram_range=(1,2) \n",
    "                                              ), 'title'),\n",
    "        ('BagOfWords2', CountVectorizer(tokenizer= StemmerTokenizer(),\n",
    "                                               ngram_range=(1,2) \n",
    "                                              ), 'overview'),\n",
    "        ('BagOfWords3', CountVectorizer(tokenizer= StemmerTokenizer(),\n",
    "                                               ngram_range=(1,2) \n",
    "                                              ), 'keywords')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ba915",
   "metadata": {},
   "source": [
    "Separamos los datos nuevamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "267437c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data[data['tipo'] == 'val'].drop('tipo', axis=1)\n",
    "data = data[data['tipo'] != 'val'].drop('tipo', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1183885",
   "metadata": {},
   "source": [
    "Hacemos LabelEncoder para que XGBoost pueda soportarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8e416f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(data['label'])\n",
    "data['label'] = le.transform(data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58498dcc",
   "metadata": {},
   "source": [
    "Razonamiento:\n",
    "\n",
    "Con las variables numéricas se aplicó una estandarización para lograr crear una distribución más suave y que evitara outliers. Además, para el texto se crearon variables como la cantidad de géneros, de actores, de productoras, etc, con tal de extraer información de las columnas que mostraban muchas categorías al mismo tiempo. Para el texto nato, se decidió aplicar un stemmizador para vectorizar y así extraer información de dichas palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5550866f",
   "metadata": {
    "cell_id": "00018-d6de5b4a-3ce2-4aaa-9421-121c4fcaf3b5",
    "deepnote_cell_height": 117.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Clasificación\n",
    "\n",
    "### 4.1 Dummy y Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e64e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b8e5c",
   "metadata": {},
   "source": [
    "Separamos los datos en train y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c64dbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(['target', 'credits', 'genres', 'tagline',\n",
    "       'original_language', 'production_companies', 'release_date', 'label'], axis=1),\n",
    "    data['label'],\n",
    "    shuffle = True,\n",
    "    test_size = 0.3,\n",
    "    random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85637e",
   "metadata": {},
   "source": [
    "Creamos el baseline y un modelo simple, en este caso un DecisionTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8e48d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_clf_baseline = Pipeline(steps = [\n",
    "    ('prepro', preprocessing),\n",
    "    ('modelo', DummyClassifier(strategy = 'stratified'))\n",
    "])\n",
    "\n",
    "pipe_clf_simplemodel = Pipeline(steps = [\n",
    "    ('prepro', preprocessing),\n",
    "    ('modelo', DecisionTreeClassifier(random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b28763d",
   "metadata": {},
   "source": [
    "Ejecutamos y vemos las métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09367b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para el modelo Baseline el performance es el siguiente:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.22      0.21      0.21       417\n",
      "Mostly Positive       0.46      0.45      0.45       908\n",
      "       Negative       0.02      0.02      0.02        47\n",
      "       Positive       0.27      0.28      0.27       514\n",
      "  Very Positive       0.06      0.06      0.06        50\n",
      "\n",
      "       accuracy                           0.33      1936\n",
      "      macro avg       0.20      0.20      0.20      1936\n",
      "   weighted avg       0.33      0.33      0.33      1936\n",
      " \n",
      "\n",
      "\n",
      "Para el modelo DecisionTree el performance es el siguiente:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.31      0.27      0.29       417\n",
      "Mostly Positive       0.49      0.56      0.52       908\n",
      "       Negative       0.03      0.02      0.03        47\n",
      "       Positive       0.41      0.40      0.41       514\n",
      "  Very Positive       0.08      0.04      0.05        50\n",
      "\n",
      "       accuracy                           0.43      1936\n",
      "      macro avg       0.27      0.26      0.26      1936\n",
      "   weighted avg       0.41      0.43      0.42      1936\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelines = {'Baseline': pipe_clf_baseline,\n",
    "            'DecisionTree': pipe_clf_simplemodel}\n",
    "\n",
    "for name, model in pipelines.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = le.inverse_transform(model.predict(X_test))\n",
    "    \n",
    "    scores = classification_report(le.inverse_transform(y_test), y_pred)\n",
    "    print(f'Para el modelo {name} el performance es el siguiente:\\n',scores,'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ef545",
   "metadata": {},
   "source": [
    "El DecisionTree es indudablemente mejor que el modelo Dummy ya que permite identificar con mayor exactitud las clases \"Positive\", \"Mixed\" y \"Mostly Positive\". Sin embargo, la mejora de la métrica \"f1-macro\" no es enorme por lo que existe un gran espacio de mejora para probar modelos más complejos y probar combinaciones de hiper-parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03670a8d",
   "metadata": {
    "cell_id": "00021-c294ef41-853d-4297-b051-d5d4e6577715",
    "deepnote_cell_height": 61.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### 4.2 Búsqueda del mejor modelo de Clasificación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc278b9e",
   "metadata": {},
   "source": [
    "Creamos la pipeline para el GridSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d6b0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_clf_grid = Pipeline(steps = [\n",
    "    ('prepro', preprocessing),\n",
    "    ('modelo', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a75a05",
   "metadata": {},
   "source": [
    "Generamos los parámetros a probar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "392878fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {\n",
    "        'modelo': [LogisticRegression(random_state = 0)],\n",
    "        'modelo__penalty': ['l1', 'l2'],\n",
    "        'modelo__solver': ['liblinear'],\n",
    "        'prepro__BagOfWords2__ngram_range': [(1,1), (1,2)],\n",
    "        'prepro__BagOfWords3__ngram_range': [(1,1), (1,2)]\n",
    "    },{\n",
    "        'modelo': [RandomForestClassifier(random_state=0)],\n",
    "        'modelo__max_depth': [2,4],\n",
    "        'prepro__BagOfWords2__ngram_range': [(1,1), (1,2)]\n",
    "    },{\n",
    "        'modelo': [xgb.XGBClassifier(random_state=0)],\n",
    "        'prepro__BagOfWords2__ngram_range': [(1,1), (1,2)],\n",
    "        'prepro__BagOfWords3__ngram_range': [(1,1), (1,2)]\n",
    "    },{\n",
    "        'modelo': [KNeighborsClassifier()],\n",
    "        'modelo__n_neighbors': [2,3,4],\n",
    "        'prepro__BagOfWords2__ngram_range': [(1,1), (1,2)],\n",
    "        'prepro__BagOfWords3__ngram_range': [(1,1), (1,2)]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ab839",
   "metadata": {},
   "source": [
    "Entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1df2175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
      "[CV 1/3; 1/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 1/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.250 total time=   9.8s\n",
      "[CV 2/3; 1/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 1/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.246 total time=   9.3s\n",
      "[CV 3/3; 1/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 1/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.240 total time=  10.5s\n",
      "[CV 1/3; 2/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 2/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.250 total time=  10.5s\n",
      "[CV 2/3; 2/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 2/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.246 total time=  10.0s\n",
      "[CV 3/3; 2/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 2/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.238 total time=  10.6s\n",
      "[CV 1/3; 3/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 3/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.245 total time=  11.6s\n",
      "[CV 2/3; 3/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 3/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.242 total time=  11.1s\n",
      "[CV 3/3; 3/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 3/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.243 total time=  11.4s\n",
      "[CV 1/3; 4/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 4/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.245 total time=  11.8s\n",
      "[CV 2/3; 4/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 4/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.243 total time=  12.0s\n",
      "[CV 3/3; 4/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 4/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l1, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.240 total time=  11.1s\n",
      "[CV 1/3; 5/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 5/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.260 total time=  10.6s\n",
      "[CV 2/3; 5/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 5/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.252 total time=  10.5s\n",
      "[CV 3/3; 5/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 5/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.245 total time=   9.9s\n",
      "[CV 1/3; 6/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 6/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.257 total time=  10.9s\n",
      "[CV 2/3; 6/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 6/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.252 total time=  11.9s\n",
      "[CV 3/3; 6/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 6/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.246 total time=  10.1s\n",
      "[CV 1/3; 7/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 7/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.252 total time=  11.6s\n",
      "[CV 2/3; 7/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 7/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.246 total time=  11.6s\n",
      "[CV 3/3; 7/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 7/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.237 total time=  11.8s\n",
      "[CV 1/3; 8/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 8/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.253 total time=  11.2s\n",
      "[CV 2/3; 8/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 8/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.250 total time=  12.0s\n",
      "[CV 3/3; 8/28] START modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 8/28] END modelo=LogisticRegression(random_state=0), modelo__penalty=l2, modelo__solver=liblinear, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.240 total time=  11.7s\n",
      "[CV 1/3; 9/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1)\n",
      "[CV 1/3; 9/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1);, score=0.126 total time=   9.8s\n",
      "[CV 2/3; 9/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1)\n",
      "[CV 2/3; 9/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1);, score=0.126 total time=  10.3s\n",
      "[CV 3/3; 9/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1)\n",
      "[CV 3/3; 9/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1);, score=0.126 total time=  10.4s\n",
      "[CV 1/3; 10/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2)\n",
      "[CV 1/3; 10/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2);, score=0.126 total time=  10.2s\n",
      "[CV 2/3; 10/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2)\n",
      "[CV 2/3; 10/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2);, score=0.126 total time=  11.0s\n",
      "[CV 3/3; 10/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2)\n",
      "[CV 3/3; 10/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2);, score=0.126 total time=  11.9s\n",
      "[CV 1/3; 11/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1)\n",
      "[CV 1/3; 11/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1);, score=0.126 total time=  10.5s\n",
      "[CV 2/3; 11/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1)\n",
      "[CV 2/3; 11/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1);, score=0.126 total time=  10.3s\n",
      "[CV 3/3; 11/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1)\n",
      "[CV 3/3; 11/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1);, score=0.126 total time=  10.6s\n",
      "[CV 1/3; 12/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2)\n",
      "[CV 1/3; 12/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2);, score=0.126 total time=  10.3s\n",
      "[CV 2/3; 12/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2)\n",
      "[CV 2/3; 12/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2);, score=0.126 total time=  10.6s\n",
      "[CV 3/3; 12/28] START modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2)\n",
      "[CV 3/3; 12/28] END modelo=RandomForestClassifier(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2);, score=0.126 total time=  10.9s\n",
      "[CV 1/3; 13/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 13/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.265 total time=  19.1s\n",
      "[CV 2/3; 13/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 13/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.250 total time=  18.7s\n",
      "[CV 3/3; 13/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 13/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.256 total time=  19.1s\n",
      "[CV 1/3; 14/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 14/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.257 total time=  22.6s\n",
      "[CV 2/3; 14/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 14/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.259 total time=  20.9s\n",
      "[CV 3/3; 14/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 14/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.266 total time=  21.0s\n",
      "[CV 1/3; 15/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 15/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.266 total time=  32.5s\n",
      "[CV 2/3; 15/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 15/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.257 total time=  33.5s\n",
      "[CV 3/3; 15/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 15/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.270 total time=  32.5s\n",
      "[CV 1/3; 16/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 16/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.269 total time=  34.9s\n",
      "[CV 2/3; 16/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 16/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.265 total time=  34.4s\n",
      "[CV 3/3; 16/28] START modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 16/28] END modelo=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=0,\n",
      "              reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.267 total time=  35.1s\n",
      "[CV 1/3; 17/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 17/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.185 total time=  12.0s\n",
      "[CV 2/3; 17/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 17/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.187 total time=  10.4s\n",
      "[CV 3/3; 17/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 17/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.189 total time=   9.6s\n",
      "[CV 1/3; 18/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 18/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.198 total time=  10.6s\n",
      "[CV 2/3; 18/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 18/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.177 total time=  11.5s\n",
      "[CV 3/3; 18/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 18/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.185 total time=   9.8s\n",
      "[CV 1/3; 19/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 19/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.188 total time=  10.9s\n",
      "[CV 2/3; 19/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 19/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.162 total time=  10.8s\n",
      "[CV 3/3; 19/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 19/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.180 total time=  10.4s\n",
      "[CV 1/3; 20/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 20/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.186 total time=  11.1s\n",
      "[CV 2/3; 20/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 20/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.165 total time=  11.0s\n",
      "[CV 3/3; 20/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 20/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.190 total time=  10.1s\n",
      "[CV 1/3; 21/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 21/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.202 total time=  10.5s\n",
      "[CV 2/3; 21/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 21/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.201 total time=  10.4s\n",
      "[CV 3/3; 21/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 21/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.200 total time=   9.7s\n",
      "[CV 1/3; 22/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 22/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.194 total time=  11.0s\n",
      "[CV 2/3; 22/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 22/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.198 total time=  10.5s\n",
      "[CV 3/3; 22/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 22/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.198 total time=   9.7s\n",
      "[CV 1/3; 23/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 23/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.211 total time=  10.8s\n",
      "[CV 2/3; 23/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 23/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.181 total time=  10.9s\n",
      "[CV 3/3; 23/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 23/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.175 total time=  10.2s\n",
      "[CV 1/3; 24/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 24/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.201 total time=  11.0s\n",
      "[CV 2/3; 24/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 24/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.180 total time=  10.8s\n",
      "[CV 3/3; 24/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 24/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=3, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.192 total time=  10.1s\n",
      "[CV 1/3; 25/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 25/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.196 total time=  10.3s\n",
      "[CV 2/3; 25/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 25/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.194 total time=  10.0s\n",
      "[CV 3/3; 25/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 25/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.199 total time=  10.0s\n",
      "[CV 1/3; 26/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 26/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.200 total time=  10.6s\n",
      "[CV 2/3; 26/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 26/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.187 total time=   9.7s\n",
      "[CV 3/3; 26/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 26/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.198 total time=  10.3s\n",
      "[CV 1/3; 27/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 27/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.201 total time=  10.8s\n",
      "[CV 2/3; 27/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 27/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.176 total time=  10.2s\n",
      "[CV 3/3; 27/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 27/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.185 total time=  11.4s\n",
      "[CV 1/3; 28/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 28/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.197 total time=  12.3s\n",
      "[CV 2/3; 28/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 28/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.169 total time=  10.9s\n",
      "[CV 3/3; 28/28] START modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 28/28] END modelo=KNeighborsClassifier(), modelo__n_neighbors=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.201 total time=  10.3s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;prepro&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;StandardScaler&#x27;,\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         [&#x27;budget&#x27;,\n",
       "                                                                          &#x27;runtime&#x27;,\n",
       "                                                                          &#x27;log_budget&#x27;]),\n",
       "                                                                        (&#x27;OneHotEncoder&#x27;,\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         [&#x27;day_name&#x27;]),\n",
       "                                                                        (&#x27;BagOfWords1&#x27;,\n",
       "                                                                         CountVectorizer(ngram_range=(1,\n",
       "                                                                                                      2),\n",
       "                                                                                         tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D28E0&gt;),\n",
       "                                                                         &#x27;title&#x27;),\n",
       "                                                                        (&#x27;BagOfWords2&#x27;,\n",
       "                                                                         CountVect...\n",
       "                                                   predictor=None,\n",
       "                                                   random_state=0,\n",
       "                                                   reg_alpha=None,\n",
       "                                                   reg_lambda=None, ...)],\n",
       "                          &#x27;prepro__BagOfWords2__ngram_range&#x27;: [(1, 1), (1, 2)],\n",
       "                          &#x27;prepro__BagOfWords3__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "                         {&#x27;modelo&#x27;: [KNeighborsClassifier()],\n",
       "                          &#x27;modelo__n_neighbors&#x27;: [2, 3, 4],\n",
       "                          &#x27;prepro__BagOfWords2__ngram_range&#x27;: [(1, 1), (1, 2)],\n",
       "                          &#x27;prepro__BagOfWords3__ngram_range&#x27;: [(1, 1),\n",
       "                                                               (1, 2)]}],\n",
       "             scoring=&#x27;f1_macro&#x27;, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;prepro&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;StandardScaler&#x27;,\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         [&#x27;budget&#x27;,\n",
       "                                                                          &#x27;runtime&#x27;,\n",
       "                                                                          &#x27;log_budget&#x27;]),\n",
       "                                                                        (&#x27;OneHotEncoder&#x27;,\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         [&#x27;day_name&#x27;]),\n",
       "                                                                        (&#x27;BagOfWords1&#x27;,\n",
       "                                                                         CountVectorizer(ngram_range=(1,\n",
       "                                                                                                      2),\n",
       "                                                                                         tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D28E0&gt;),\n",
       "                                                                         &#x27;title&#x27;),\n",
       "                                                                        (&#x27;BagOfWords2&#x27;,\n",
       "                                                                         CountVect...\n",
       "                                                   predictor=None,\n",
       "                                                   random_state=0,\n",
       "                                                   reg_alpha=None,\n",
       "                                                   reg_lambda=None, ...)],\n",
       "                          &#x27;prepro__BagOfWords2__ngram_range&#x27;: [(1, 1), (1, 2)],\n",
       "                          &#x27;prepro__BagOfWords3__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "                         {&#x27;modelo&#x27;: [KNeighborsClassifier()],\n",
       "                          &#x27;modelo__n_neighbors&#x27;: [2, 3, 4],\n",
       "                          &#x27;prepro__BagOfWords2__ngram_range&#x27;: [(1, 1), (1, 2)],\n",
       "                          &#x27;prepro__BagOfWords3__ngram_range&#x27;: [(1, 1),\n",
       "                                                               (1, 2)]}],\n",
       "             scoring=&#x27;f1_macro&#x27;, verbose=10)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;prepro&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;StandardScaler&#x27;,\n",
       "                                                  StandardScaler(),\n",
       "                                                  [&#x27;budget&#x27;, &#x27;runtime&#x27;,\n",
       "                                                   &#x27;log_budget&#x27;]),\n",
       "                                                 (&#x27;OneHotEncoder&#x27;,\n",
       "                                                  OneHotEncoder(),\n",
       "                                                  [&#x27;day_name&#x27;]),\n",
       "                                                 (&#x27;BagOfWords1&#x27;,\n",
       "                                                  CountVectorizer(ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D28E0&gt;),\n",
       "                                                  &#x27;title&#x27;),\n",
       "                                                 (&#x27;BagOfWords2&#x27;,\n",
       "                                                  CountVectorizer(ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2A90&gt;),\n",
       "                                                  &#x27;overview&#x27;),\n",
       "                                                 (&#x27;BagOfWords3&#x27;,\n",
       "                                                  CountVectorizer(ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2CA0&gt;),\n",
       "                                                  &#x27;keywords&#x27;)])),\n",
       "                (&#x27;modelo&#x27;, DecisionTreeClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">prepro: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;StandardScaler&#x27;, StandardScaler(),\n",
       "                                 [&#x27;budget&#x27;, &#x27;runtime&#x27;, &#x27;log_budget&#x27;]),\n",
       "                                (&#x27;OneHotEncoder&#x27;, OneHotEncoder(),\n",
       "                                 [&#x27;day_name&#x27;]),\n",
       "                                (&#x27;BagOfWords1&#x27;,\n",
       "                                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                                 tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D28E0&gt;),\n",
       "                                 &#x27;title&#x27;),\n",
       "                                (&#x27;BagOfWords2&#x27;,\n",
       "                                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                                 tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2A90&gt;),\n",
       "                                 &#x27;overview&#x27;),\n",
       "                                (&#x27;BagOfWords3&#x27;,\n",
       "                                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                                 tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2CA0&gt;),\n",
       "                                 &#x27;keywords&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>[&#x27;budget&#x27;, &#x27;runtime&#x27;, &#x27;log_budget&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;day_name&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BagOfWords1</label><div class=\"sk-toggleable__content\"><pre>title</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2),\n",
       "                tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D28E0&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BagOfWords2</label><div class=\"sk-toggleable__content\"><pre>overview</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2),\n",
       "                tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2A90&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BagOfWords3</label><div class=\"sk-toggleable__content\"><pre>keywords</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2),\n",
       "                tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2CA0&gt;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('prepro',\n",
       "                                        ColumnTransformer(transformers=[('StandardScaler',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['budget',\n",
       "                                                                          'runtime',\n",
       "                                                                          'log_budget']),\n",
       "                                                                        ('OneHotEncoder',\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         ['day_name']),\n",
       "                                                                        ('BagOfWords1',\n",
       "                                                                         CountVectorizer(ngram_range=(1,\n",
       "                                                                                                      2),\n",
       "                                                                                         tokenizer=<__main__.StemmerTokenizer object at 0x000001AAAB6D28E0>),\n",
       "                                                                         'title'),\n",
       "                                                                        ('BagOfWords2',\n",
       "                                                                         CountVect...\n",
       "                                                   predictor=None,\n",
       "                                                   random_state=0,\n",
       "                                                   reg_alpha=None,\n",
       "                                                   reg_lambda=None, ...)],\n",
       "                          'prepro__BagOfWords2__ngram_range': [(1, 1), (1, 2)],\n",
       "                          'prepro__BagOfWords3__ngram_range': [(1, 1), (1, 2)]},\n",
       "                         {'modelo': [KNeighborsClassifier()],\n",
       "                          'modelo__n_neighbors': [2, 3, 4],\n",
       "                          'prepro__BagOfWords2__ngram_range': [(1, 1), (1, 2)],\n",
       "                          'prepro__BagOfWords3__ngram_range': [(1, 1),\n",
       "                                                               (1, 2)]}],\n",
       "             scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model_clf = GridSearchCV(pipe_clf_grid,\n",
    "                          param_grid=params,\n",
    "                          verbose=10,\n",
    "                          scoring = 'f1_macro',\n",
    "                          cv=3)\n",
    "\n",
    "grid_model_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f608821",
   "metadata": {},
   "source": [
    "Visualizamos el mejor modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acfdac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.37      0.16      0.22       417\n",
      "Mostly Positive       0.50      0.73      0.59       908\n",
      "       Negative       0.00      0.00      0.00        47\n",
      "       Positive       0.50      0.42      0.46       514\n",
      "  Very Positive       0.00      0.00      0.00        50\n",
      "\n",
      "       accuracy                           0.49      1936\n",
      "      macro avg       0.27      0.26      0.25      1936\n",
      "   weighted avg       0.45      0.49      0.45      1936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = le.inverse_transform(grid_model_clf.predict(X_test))\n",
    "print(classification_report(le.inverse_transform(y_test), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ed52a",
   "metadata": {},
   "source": [
    "El TOP 5 de modelos encontrados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bf9ba1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_modelo</th>\n",
       "      <th>param_modelo__penalty</th>\n",
       "      <th>param_modelo__solver</th>\n",
       "      <th>param_prepro__BagOfWords2__ngram_range</th>\n",
       "      <th>param_prepro__BagOfWords3__ngram_range</th>\n",
       "      <th>param_modelo__max_depth</th>\n",
       "      <th>param_modelo__n_neighbors</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>31.077326</td>\n",
       "      <td>0.134227</td>\n",
       "      <td>3.785813</td>\n",
       "      <td>0.304545</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'modelo': XGBClassifier(base_score=None, boos...</td>\n",
       "      <td>0.268943</td>\n",
       "      <td>0.265005</td>\n",
       "      <td>0.266767</td>\n",
       "      <td>0.266905</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>28.727118</td>\n",
       "      <td>0.444869</td>\n",
       "      <td>4.202138</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'modelo': XGBClassifier(base_score=None, boos...</td>\n",
       "      <td>0.265651</td>\n",
       "      <td>0.257181</td>\n",
       "      <td>0.269544</td>\n",
       "      <td>0.264125</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18.139403</td>\n",
       "      <td>0.752783</td>\n",
       "      <td>3.468434</td>\n",
       "      <td>0.109548</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'modelo': XGBClassifier(base_score=None, boos...</td>\n",
       "      <td>0.256727</td>\n",
       "      <td>0.258870</td>\n",
       "      <td>0.266415</td>\n",
       "      <td>0.260671</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.595951</td>\n",
       "      <td>0.181515</td>\n",
       "      <td>3.478224</td>\n",
       "      <td>0.191650</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'modelo': XGBClassifier(base_score=None, boos...</td>\n",
       "      <td>0.265333</td>\n",
       "      <td>0.249660</td>\n",
       "      <td>0.255944</td>\n",
       "      <td>0.256979</td>\n",
       "      <td>0.006440</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.164072</td>\n",
       "      <td>0.285320</td>\n",
       "      <td>3.306566</td>\n",
       "      <td>0.080994</td>\n",
       "      <td>LogisticRegression(random_state=0)</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'modelo': LogisticRegression(random_state=0),...</td>\n",
       "      <td>0.259821</td>\n",
       "      <td>0.251751</td>\n",
       "      <td>0.244822</td>\n",
       "      <td>0.252131</td>\n",
       "      <td>0.006129</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "15      31.077326      0.134227         3.785813        0.304545   \n",
       "14      28.727118      0.444869         4.202138        0.096296   \n",
       "13      18.139403      0.752783         3.468434        0.109548   \n",
       "12      15.595951      0.181515         3.478224        0.191650   \n",
       "4        7.164072      0.285320         3.306566        0.080994   \n",
       "\n",
       "                                         param_modelo param_modelo__penalty  \\\n",
       "15  XGBClassifier(base_score=None, booster=None, c...                   NaN   \n",
       "14  XGBClassifier(base_score=None, booster=None, c...                   NaN   \n",
       "13  XGBClassifier(base_score=None, booster=None, c...                   NaN   \n",
       "12  XGBClassifier(base_score=None, booster=None, c...                   NaN   \n",
       "4                  LogisticRegression(random_state=0)                    l2   \n",
       "\n",
       "   param_modelo__solver param_prepro__BagOfWords2__ngram_range  \\\n",
       "15                  NaN                                 (1, 2)   \n",
       "14                  NaN                                 (1, 2)   \n",
       "13                  NaN                                 (1, 1)   \n",
       "12                  NaN                                 (1, 1)   \n",
       "4             liblinear                                 (1, 1)   \n",
       "\n",
       "   param_prepro__BagOfWords3__ngram_range param_modelo__max_depth  \\\n",
       "15                                 (1, 2)                     NaN   \n",
       "14                                 (1, 1)                     NaN   \n",
       "13                                 (1, 2)                     NaN   \n",
       "12                                 (1, 1)                     NaN   \n",
       "4                                  (1, 1)                     NaN   \n",
       "\n",
       "   param_modelo__n_neighbors  \\\n",
       "15                       NaN   \n",
       "14                       NaN   \n",
       "13                       NaN   \n",
       "12                       NaN   \n",
       "4                        NaN   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "15  {'modelo': XGBClassifier(base_score=None, boos...           0.268943   \n",
       "14  {'modelo': XGBClassifier(base_score=None, boos...           0.265651   \n",
       "13  {'modelo': XGBClassifier(base_score=None, boos...           0.256727   \n",
       "12  {'modelo': XGBClassifier(base_score=None, boos...           0.265333   \n",
       "4   {'modelo': LogisticRegression(random_state=0),...           0.259821   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "15           0.265005           0.266767         0.266905        0.001611   \n",
       "14           0.257181           0.269544         0.264125        0.005161   \n",
       "13           0.258870           0.266415         0.260671        0.004155   \n",
       "12           0.249660           0.255944         0.256979        0.006440   \n",
       "4            0.251751           0.244822         0.252131        0.006129   \n",
       "\n",
       "    rank_test_score  \n",
       "15                1  \n",
       "14                2  \n",
       "13                3  \n",
       "12                4  \n",
       "4                 5  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_model_clf.cv_results_).sort_values(by='rank_test_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5247d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yc_val = grid_model_clf.predict(data_val.drop(['target', 'credits', 'genres', 'tagline',\n",
    "       'original_language', 'production_companies', 'release_date', 'label'], axis=1))\n",
    "\n",
    "yc_val = le.inverse_transform(yc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff12117",
   "metadata": {
    "cell_id": "00023-73786884-e1b2-448a-9636-ab05cf3c1fc5",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "En el GridSearch se probaron distintas configuraciones, principalmente en la forma de procesar el texto dada la cantidad de variables relacionadas, por lo cual el proceso fue muy largo y costoso computacionlmente. Los modelos probados fueron: RandomForest, XGBoost, KNeighbors y Regresión Logistica. \n",
    "\n",
    "Luego de probar cada modelo, se llega a la conclusión de que el mejor modelo es el XGBoost dado que 4 configuraciones distintas poseen los 5 mejores rendimientos en el set de testeo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e13a8",
   "metadata": {
    "cell_id": "2d58aececbe34d6184477c8e3cfaa2e3",
    "deepnote_cell_height": 117.69999694824219,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Regresión\n",
    "\n",
    "### 5.1 Dummy y Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08dbae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e5515",
   "metadata": {},
   "source": [
    "Separamos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6f44cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    data.drop(['target', 'tagline', 'credits', 'genres',\n",
    "       'original_language', 'production_companies', 'release_date', 'label'], axis=1),\n",
    "    data['target'],\n",
    "    shuffle = True,\n",
    "    test_size = 0.3,\n",
    "    random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cb162e",
   "metadata": {},
   "source": [
    "Generamos los pipelines para el baseline y el modelo simple (Regresión Lineal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0381cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_reg_baseline = Pipeline(steps = [\n",
    "    ('prepro', preprocessing),\n",
    "    ('model', DummyRegressor())\n",
    "])\n",
    "\n",
    "pipe_reg_simplemodel = Pipeline(steps = [\n",
    "    ('prepro', preprocessing),\n",
    "    ('model', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a82101b",
   "metadata": {},
   "source": [
    "Entrenamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fafd9749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para el modelo Baseline el performance es el siguiente:\n",
      " -0.0 \n",
      "\n",
      "\n",
      "Para el modelo OLS el performance es el siguiente:\n",
      " 0.525 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelines = {'Baseline':pipe_reg_baseline,\n",
    "             'OLS': pipe_reg_simplemodel}\n",
    "\n",
    "for name, model in pipelines.items():\n",
    "    model.fit(Xr_train, yr_train)\n",
    "    yr_pred = model.predict(Xr_test)\n",
    "    \n",
    "    scores = r2_score(yr_test, yr_pred)\n",
    "    print(f'Para el modelo {name} el performance es el siguiente:\\n',np.round(scores,3),'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7dee8",
   "metadata": {},
   "source": [
    "Claramente el modelo Dummy tiene un mal performance al llegar a un R2 negativo. Afortunadamente, la regresión lineal ha sido altamente competente y entrega un R2 muy alto, mayor al 0.5 lo cual es un gran performance para un modelo tan simple y que no requiere una parametrización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a74e3f7",
   "metadata": {
    "cell_id": "df491c3ed9704211be52235df236b889",
    "deepnote_cell_height": 61.69999694824219,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### 5.2 Búsqueda del mejor modelo de Regresión\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efec8a1",
   "metadata": {},
   "source": [
    "Creamos la pipeline base para el GridSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d815dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_reg_grid = Pipeline(steps = [\n",
    "    ('prepro', preprocessing),\n",
    "    ('modelo', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd78c8",
   "metadata": {},
   "source": [
    "Cambiamos parámetros de interés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96133a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {\n",
    "        'modelo': [LinearRegression()],\n",
    "        'prepro__BagOfWords2__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "        'prepro__BagOfWords3__ngram_range': [(1,1), (1,2), (1,3)]\n",
    "    },{\n",
    "        'modelo': [RandomForestRegressor(random_state=0)],\n",
    "        'modelo__max_depth': [2,4],\n",
    "        'prepro__BagOfWords2__ngram_range': [(1,1), (1,2)],\n",
    "        'prepro__BagOfWords3__ngram_range': [(1,1), (1,2)]\n",
    "    },{\n",
    "        'modelo': [xgb.XGBRegressor(random_state=0)],\n",
    "        'prepro__BagOfWords2__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "        'prepro__BagOfWords3__ngram_range': [(1,1), (1,2), (1,3)]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3ba3e",
   "metadata": {},
   "source": [
    "Entrenamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1a63c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 26 candidates, totalling 78 fits\n",
      "[CV 1/3; 1/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 1/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.466 total time=  10.1s\n",
      "[CV 2/3; 1/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 1/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.508 total time=   9.5s\n",
      "[CV 3/3; 1/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 1/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.519 total time=   9.1s\n",
      "[CV 1/3; 2/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 2/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.485 total time=  10.1s\n",
      "[CV 2/3; 2/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 2/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.535 total time=  10.7s\n",
      "[CV 3/3; 2/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 2/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.552 total time=  10.0s\n",
      "[CV 1/3; 3/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 1/3; 3/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.490 total time=  10.6s\n",
      "[CV 2/3; 3/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 2/3; 3/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.546 total time=  10.4s\n",
      "[CV 3/3; 3/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 3/3; 3/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.565 total time=  10.1s\n",
      "[CV 1/3; 4/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 4/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.511 total time=  11.2s\n",
      "[CV 2/3; 4/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 4/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.553 total time=  11.2s\n",
      "[CV 3/3; 4/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 4/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.571 total time=  10.9s\n",
      "[CV 1/3; 5/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 5/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.518 total time=  11.5s\n",
      "[CV 2/3; 5/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 5/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.561 total time=  11.7s\n",
      "[CV 3/3; 5/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 5/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.581 total time=  11.2s\n",
      "[CV 1/3; 6/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 1/3; 6/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.520 total time=  12.0s\n",
      "[CV 2/3; 6/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 2/3; 6/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.565 total time=  13.2s\n",
      "[CV 3/3; 6/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 3/3; 6/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.586 total time=  12.7s\n",
      "[CV 1/3; 7/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 7/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.522 total time=  16.1s\n",
      "[CV 2/3; 7/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 7/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.568 total time=  15.4s\n",
      "[CV 3/3; 7/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 7/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.590 total time=  14.8s\n",
      "[CV 1/3; 8/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 8/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.528 total time=  13.8s\n",
      "[CV 2/3; 8/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 8/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.572 total time=  12.8s\n",
      "[CV 3/3; 8/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 8/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.596 total time=  13.8s\n",
      "[CV 1/3; 9/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 1/3; 9/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.529 total time=  13.7s\n",
      "[CV 2/3; 9/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 2/3; 9/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.574 total time=  16.1s\n",
      "[CV 3/3; 9/26] START modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 3/3; 9/26] END modelo=LinearRegression(), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.599 total time=  14.9s\n",
      "[CV 1/3; 10/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 10/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.492 total time=  24.6s\n",
      "[CV 2/3; 10/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 10/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.540 total time=  16.0s\n",
      "[CV 3/3; 10/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 10/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.588 total time=  14.6s\n",
      "[CV 1/3; 11/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 11/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.493 total time=  13.4s\n",
      "[CV 2/3; 11/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 11/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.539 total time=  13.7s\n",
      "[CV 3/3; 11/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 11/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.588 total time=  14.3s\n",
      "[CV 1/3; 12/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 12/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.492 total time=  16.7s\n",
      "[CV 2/3; 12/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 12/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.539 total time=  16.4s\n",
      "[CV 3/3; 12/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 12/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.588 total time=  16.6s\n",
      "[CV 1/3; 13/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 13/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.492 total time=  17.4s\n",
      "[CV 2/3; 13/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 13/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.540 total time=  17.4s\n",
      "[CV 3/3; 13/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 13/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=2, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.588 total time=  17.3s\n",
      "[CV 1/3; 14/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 14/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.516 total time=  15.9s\n",
      "[CV 2/3; 14/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 14/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.577 total time=  16.2s\n",
      "[CV 3/3; 14/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 14/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.608 total time=  15.9s\n",
      "[CV 1/3; 15/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 15/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.518 total time=  17.2s\n",
      "[CV 2/3; 15/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 15/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.577 total time=  17.4s\n",
      "[CV 3/3; 15/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 15/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.609 total time=  17.0s\n",
      "[CV 1/3; 16/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 16/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.514 total time=  24.3s\n",
      "[CV 2/3; 16/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 16/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.572 total time=  25.1s\n",
      "[CV 3/3; 16/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 16/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.598 total time=  24.8s\n",
      "[CV 1/3; 17/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 17/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.515 total time=  26.5s\n",
      "[CV 2/3; 17/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 17/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.573 total time=  29.7s\n",
      "[CV 3/3; 17/26] START modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 17/26] END modelo=RandomForestRegressor(random_state=0), modelo__max_depth=4, prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.599 total time=  50.3s\n",
      "[CV 1/3; 18/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 18/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.512 total time=  16.4s\n",
      "[CV 2/3; 18/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 18/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.574 total time=  12.8s\n",
      "[CV 3/3; 18/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 18/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.605 total time=  12.8s\n",
      "[CV 1/3; 19/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 19/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.509 total time=  13.2s\n",
      "[CV 2/3; 19/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 19/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.571 total time=  13.0s\n",
      "[CV 3/3; 19/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 19/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.608 total time=  13.1s\n",
      "[CV 1/3; 20/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 1/3; 20/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.509 total time=  18.0s\n",
      "[CV 2/3; 20/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 2/3; 20/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.566 total time=  14.9s\n",
      "[CV 3/3; 20/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 3/3; 20/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 1), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.608 total time=  13.9s\n",
      "[CV 1/3; 21/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 21/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.525 total time=  16.6s\n",
      "[CV 2/3; 21/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 21/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.549 total time=  16.9s\n",
      "[CV 3/3; 21/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 3/3; 21/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.606 total time=  17.1s\n",
      "[CV 1/3; 22/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 22/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.528 total time=  18.3s\n",
      "[CV 2/3; 22/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 22/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.550 total time=  17.5s\n",
      "[CV 3/3; 22/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 22/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.608 total time=  17.4s\n",
      "[CV 1/3; 23/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3; 23/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.528 total time=  18.3s\n",
      "[CV 2/3; 23/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 2/3; 23/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.551 total time=  19.1s\n",
      "[CV 3/3; 23/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 3/3; 23/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 2), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.608 total time=  18.3s\n",
      "[CV 1/3; 24/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 1/3; 24/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.527 total time=  23.2s\n",
      "[CV 2/3; 24/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1)\n",
      "[CV 2/3; 24/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.543 total time=  22.7s\n",
      "[CV 3/3; 24/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3; 24/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 1);, score=0.602 total time=  24.9s\n",
      "[CV 1/3; 25/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 1/3; 25/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.528 total time=  24.4s\n",
      "[CV 2/3; 25/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 2/3; 25/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.541 total time=  25.4s\n",
      "[CV 3/3; 25/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2)\n",
      "[CV 3/3; 25/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 2);, score=0.603 total time=  23.5s\n",
      "[CV 1/3; 26/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 1/3; 26/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.528 total time=  25.2s\n",
      "[CV 2/3; 26/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3; 26/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.541 total time=  24.6s\n",
      "[CV 3/3; 26/26] START modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3)\n",
      "[CV 3/3; 26/26] END modelo=XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=0,\n",
      "             reg_alpha=None, reg_lambda=None, ...), prepro__BagOfWords2__ngram_range=(1, 3), prepro__BagOfWords3__ngram_range=(1, 3);, score=0.603 total time=  25.2s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;prepro&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;StandardScaler&#x27;,\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         [&#x27;budget&#x27;,\n",
       "                                                                          &#x27;runtime&#x27;,\n",
       "                                                                          &#x27;log_budget&#x27;]),\n",
       "                                                                        (&#x27;OneHotEncoder&#x27;,\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         [&#x27;day_name&#x27;]),\n",
       "                                                                        (&#x27;BagOfWords1&#x27;,\n",
       "                                                                         CountVectorizer(ngram_range=(1,\n",
       "                                                                                                      2),\n",
       "                                                                                         tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D28E0&gt;),\n",
       "                                                                         &#x27;title&#x27;),\n",
       "                                                                        (&#x27;BagOfWords2&#x27;,\n",
       "                                                                         CountVect...\n",
       "                                                  max_delta_step=None,\n",
       "                                                  max_depth=None,\n",
       "                                                  max_leaves=None,\n",
       "                                                  min_child_weight=None,\n",
       "                                                  missing=nan,\n",
       "                                                  monotone_constraints=None,\n",
       "                                                  n_estimators=100, n_jobs=None,\n",
       "                                                  num_parallel_tree=None,\n",
       "                                                  predictor=None,\n",
       "                                                  random_state=0,\n",
       "                                                  reg_alpha=None,\n",
       "                                                  reg_lambda=None, ...)],\n",
       "                          &#x27;prepro__BagOfWords2__ngram_range&#x27;: [(1, 1), (1, 2),\n",
       "                                                               (1, 3)],\n",
       "                          &#x27;prepro__BagOfWords3__ngram_range&#x27;: [(1, 1), (1, 2),\n",
       "                                                               (1, 3)]}],\n",
       "             scoring=&#x27;r2&#x27;, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;prepro&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;StandardScaler&#x27;,\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         [&#x27;budget&#x27;,\n",
       "                                                                          &#x27;runtime&#x27;,\n",
       "                                                                          &#x27;log_budget&#x27;]),\n",
       "                                                                        (&#x27;OneHotEncoder&#x27;,\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         [&#x27;day_name&#x27;]),\n",
       "                                                                        (&#x27;BagOfWords1&#x27;,\n",
       "                                                                         CountVectorizer(ngram_range=(1,\n",
       "                                                                                                      2),\n",
       "                                                                                         tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D28E0&gt;),\n",
       "                                                                         &#x27;title&#x27;),\n",
       "                                                                        (&#x27;BagOfWords2&#x27;,\n",
       "                                                                         CountVect...\n",
       "                                                  max_delta_step=None,\n",
       "                                                  max_depth=None,\n",
       "                                                  max_leaves=None,\n",
       "                                                  min_child_weight=None,\n",
       "                                                  missing=nan,\n",
       "                                                  monotone_constraints=None,\n",
       "                                                  n_estimators=100, n_jobs=None,\n",
       "                                                  num_parallel_tree=None,\n",
       "                                                  predictor=None,\n",
       "                                                  random_state=0,\n",
       "                                                  reg_alpha=None,\n",
       "                                                  reg_lambda=None, ...)],\n",
       "                          &#x27;prepro__BagOfWords2__ngram_range&#x27;: [(1, 1), (1, 2),\n",
       "                                                               (1, 3)],\n",
       "                          &#x27;prepro__BagOfWords3__ngram_range&#x27;: [(1, 1), (1, 2),\n",
       "                                                               (1, 3)]}],\n",
       "             scoring=&#x27;r2&#x27;, verbose=10)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;prepro&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;StandardScaler&#x27;,\n",
       "                                                  StandardScaler(),\n",
       "                                                  [&#x27;budget&#x27;, &#x27;runtime&#x27;,\n",
       "                                                   &#x27;log_budget&#x27;]),\n",
       "                                                 (&#x27;OneHotEncoder&#x27;,\n",
       "                                                  OneHotEncoder(),\n",
       "                                                  [&#x27;day_name&#x27;]),\n",
       "                                                 (&#x27;BagOfWords1&#x27;,\n",
       "                                                  CountVectorizer(ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D28E0&gt;),\n",
       "                                                  &#x27;title&#x27;),\n",
       "                                                 (&#x27;BagOfWords2&#x27;,\n",
       "                                                  CountVectorizer(ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2A90&gt;),\n",
       "                                                  &#x27;overview&#x27;),\n",
       "                                                 (&#x27;BagOfWords3&#x27;,\n",
       "                                                  CountVectorizer(ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2CA0&gt;),\n",
       "                                                  &#x27;keywords&#x27;)])),\n",
       "                (&#x27;modelo&#x27;, LinearRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">prepro: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;StandardScaler&#x27;, StandardScaler(),\n",
       "                                 [&#x27;budget&#x27;, &#x27;runtime&#x27;, &#x27;log_budget&#x27;]),\n",
       "                                (&#x27;OneHotEncoder&#x27;, OneHotEncoder(),\n",
       "                                 [&#x27;day_name&#x27;]),\n",
       "                                (&#x27;BagOfWords1&#x27;,\n",
       "                                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                                 tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D28E0&gt;),\n",
       "                                 &#x27;title&#x27;),\n",
       "                                (&#x27;BagOfWords2&#x27;,\n",
       "                                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                                 tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2A90&gt;),\n",
       "                                 &#x27;overview&#x27;),\n",
       "                                (&#x27;BagOfWords3&#x27;,\n",
       "                                 CountVectorizer(ngram_range=(1, 2),\n",
       "                                                 tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2CA0&gt;),\n",
       "                                 &#x27;keywords&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>[&#x27;budget&#x27;, &#x27;runtime&#x27;, &#x27;log_budget&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;day_name&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BagOfWords1</label><div class=\"sk-toggleable__content\"><pre>title</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2),\n",
       "                tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D28E0&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BagOfWords2</label><div class=\"sk-toggleable__content\"><pre>overview</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2),\n",
       "                tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2A90&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BagOfWords3</label><div class=\"sk-toggleable__content\"><pre>keywords</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2),\n",
       "                tokenizer=&lt;__main__.StemmerTokenizer object at 0x000001AAAB6D2CA0&gt;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('prepro',\n",
       "                                        ColumnTransformer(transformers=[('StandardScaler',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['budget',\n",
       "                                                                          'runtime',\n",
       "                                                                          'log_budget']),\n",
       "                                                                        ('OneHotEncoder',\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         ['day_name']),\n",
       "                                                                        ('BagOfWords1',\n",
       "                                                                         CountVectorizer(ngram_range=(1,\n",
       "                                                                                                      2),\n",
       "                                                                                         tokenizer=<__main__.StemmerTokenizer object at 0x000001AAAB6D28E0>),\n",
       "                                                                         'title'),\n",
       "                                                                        ('BagOfWords2',\n",
       "                                                                         CountVect...\n",
       "                                                  max_delta_step=None,\n",
       "                                                  max_depth=None,\n",
       "                                                  max_leaves=None,\n",
       "                                                  min_child_weight=None,\n",
       "                                                  missing=nan,\n",
       "                                                  monotone_constraints=None,\n",
       "                                                  n_estimators=100, n_jobs=None,\n",
       "                                                  num_parallel_tree=None,\n",
       "                                                  predictor=None,\n",
       "                                                  random_state=0,\n",
       "                                                  reg_alpha=None,\n",
       "                                                  reg_lambda=None, ...)],\n",
       "                          'prepro__BagOfWords2__ngram_range': [(1, 1), (1, 2),\n",
       "                                                               (1, 3)],\n",
       "                          'prepro__BagOfWords3__ngram_range': [(1, 1), (1, 2),\n",
       "                                                               (1, 3)]}],\n",
       "             scoring='r2', verbose=10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model_reg = GridSearchCV(pipe_reg_grid,\n",
    "                          param_grid=params,\n",
    "                          verbose=10,\n",
    "                          scoring = 'r2',\n",
    "                          cv=3)\n",
    "\n",
    "grid_model_reg.fit(Xr_train, yr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ac337",
   "metadata": {},
   "source": [
    "Vemos el score del mejor modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b74a1f2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_modelo</th>\n",
       "      <th>param_prepro__BagOfWords2__ngram_range</th>\n",
       "      <th>param_prepro__BagOfWords3__ngram_range</th>\n",
       "      <th>param_modelo__max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.039904</td>\n",
       "      <td>0.257480</td>\n",
       "      <td>3.286812</td>\n",
       "      <td>0.205433</td>\n",
       "      <td>RandomForestRegressor(max_depth=4, random_stat...</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>4</td>\n",
       "      <td>{'modelo': RandomForestRegressor(max_depth=4, ...</td>\n",
       "      <td>0.517888</td>\n",
       "      <td>0.576925</td>\n",
       "      <td>0.609210</td>\n",
       "      <td>0.568008</td>\n",
       "      <td>0.037811</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.084009</td>\n",
       "      <td>0.605124</td>\n",
       "      <td>3.927984</td>\n",
       "      <td>0.564064</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'modelo': LinearRegression(), 'prepro__BagOfW...</td>\n",
       "      <td>0.529381</td>\n",
       "      <td>0.574226</td>\n",
       "      <td>0.598862</td>\n",
       "      <td>0.567490</td>\n",
       "      <td>0.028763</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12.267656</td>\n",
       "      <td>0.118202</td>\n",
       "      <td>3.816814</td>\n",
       "      <td>0.072554</td>\n",
       "      <td>RandomForestRegressor(max_depth=4, random_stat...</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>4</td>\n",
       "      <td>{'modelo': RandomForestRegressor(max_depth=4, ...</td>\n",
       "      <td>0.516457</td>\n",
       "      <td>0.576731</td>\n",
       "      <td>0.608192</td>\n",
       "      <td>0.567127</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.721029</td>\n",
       "      <td>0.410769</td>\n",
       "      <td>3.844726</td>\n",
       "      <td>0.429512</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'modelo': LinearRegression(), 'prepro__BagOfW...</td>\n",
       "      <td>0.527662</td>\n",
       "      <td>0.571952</td>\n",
       "      <td>0.596060</td>\n",
       "      <td>0.565225</td>\n",
       "      <td>0.028326</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.707795</td>\n",
       "      <td>1.575881</td>\n",
       "      <td>3.379925</td>\n",
       "      <td>0.145755</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'modelo': XGBRegressor(base_score=None, boost...</td>\n",
       "      <td>0.512233</td>\n",
       "      <td>0.573618</td>\n",
       "      <td>0.604603</td>\n",
       "      <td>0.563485</td>\n",
       "      <td>0.038385</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "14      14.039904      0.257480         3.286812        0.205433   \n",
       "8       11.084009      0.605124         3.927984        0.564064   \n",
       "13      12.267656      0.118202         3.816814        0.072554   \n",
       "7        9.721029      0.410769         3.844726        0.429512   \n",
       "17      10.707795      1.575881         3.379925        0.145755   \n",
       "\n",
       "                                         param_modelo  \\\n",
       "14  RandomForestRegressor(max_depth=4, random_stat...   \n",
       "8                                  LinearRegression()   \n",
       "13  RandomForestRegressor(max_depth=4, random_stat...   \n",
       "7                                  LinearRegression()   \n",
       "17  XGBRegressor(base_score=None, booster=None, ca...   \n",
       "\n",
       "   param_prepro__BagOfWords2__ngram_range  \\\n",
       "14                                 (1, 1)   \n",
       "8                                  (1, 3)   \n",
       "13                                 (1, 1)   \n",
       "7                                  (1, 3)   \n",
       "17                                 (1, 1)   \n",
       "\n",
       "   param_prepro__BagOfWords3__ngram_range param_modelo__max_depth  \\\n",
       "14                                 (1, 2)                       4   \n",
       "8                                  (1, 3)                     NaN   \n",
       "13                                 (1, 1)                       4   \n",
       "7                                  (1, 2)                     NaN   \n",
       "17                                 (1, 1)                     NaN   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "14  {'modelo': RandomForestRegressor(max_depth=4, ...           0.517888   \n",
       "8   {'modelo': LinearRegression(), 'prepro__BagOfW...           0.529381   \n",
       "13  {'modelo': RandomForestRegressor(max_depth=4, ...           0.516457   \n",
       "7   {'modelo': LinearRegression(), 'prepro__BagOfW...           0.527662   \n",
       "17  {'modelo': XGBRegressor(base_score=None, boost...           0.512233   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "14           0.576925           0.609210         0.568008        0.037811   \n",
       "8            0.574226           0.598862         0.567490        0.028763   \n",
       "13           0.576731           0.608192         0.567127        0.038062   \n",
       "7            0.571952           0.596060         0.565225        0.028326   \n",
       "17           0.573618           0.604603         0.563485        0.038385   \n",
       "\n",
       "    rank_test_score  \n",
       "14                1  \n",
       "8                 2  \n",
       "13                3  \n",
       "7                 4  \n",
       "17                5  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_model_reg.cv_results_).sort_values('rank_test_score').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a4a5a",
   "metadata": {},
   "source": [
    "Tomamos el modelo que tiene un mejor performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8dd9691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5371998630857908\n"
     ]
    }
   ],
   "source": [
    "pipe_reg_grid_best = Pipeline(steps = [\n",
    "    ('prepro', preprocessing),\n",
    "    ('modelo', LinearRegression())\n",
    "])\n",
    "\n",
    "pipe_reg_grid_best.set_params(prepro__BagOfWords2__ngram_range=(1,3))\n",
    "pipe_reg_grid_best.set_params(prepro__BagOfWords3__ngram_range=(1,3))\n",
    "\n",
    "pipe_reg_grid_best.fit(Xr_train, yr_train)\n",
    "yr_pred = pipe_reg_grid_best.predict(Xr_test)\n",
    "\n",
    "print(r2_score(yr_test, yr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9575b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_val = pipe_reg_grid_best.predict(data_val.drop(['target', 'credits', 'genres',\n",
    "                                       'original_language', 'production_companies',\n",
    "                                      'release_date', 'label'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e20da",
   "metadata": {},
   "source": [
    "La regresión lineal trae unos grandes resultados en comparación a modelos mucho más complejos y costosos, por lo que se opta por ese modelo y no por el RandomForest que tiene una performance milimétricamente mayor. Los mejores modelos oscilan en un R2 cercano a 0.59 en el set de testo por lo cual se mejora lo obtenido en la sección anterior, sin embargo el coste computacional de encontrar los parámetros fue muy alto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba839c6",
   "metadata": {
    "cell_id": "00025-2acf9c12-da85-4c1f-add5-f2b0f600177f",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 6. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48bbba",
   "metadata": {
    "cell_id": "00026-15c07b20-0e16-48fa-bf3c-33aeb2c4c1db",
    "deepnote_cell_height": 51.53334045410156,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "El problema de negocio era generar modelos de predicción para variables categóricas (clasificación) y numéricas (regresión). Con los rendimientos obtenidos en las secciones anteriores se puede decir que **si se cumplieron con los objetivos** planteados. En todos los casos, el modelo genera un output que es mejor que la asignación aleatoria y que además es mejor que el baseline de la competencia, por lo que los resultados son aceptables.\n",
    "\n",
    "La generación de estos modelos comenzó con el análisis exploratorio, en donde se comprendieron las diversas relaciones que existían entre las variables, principalmente aquellos atributos que denotaban texto. De esta forma se pudo simplificar cierta información y generar nuevas variables que ayudaran a resolver el problema. Uno de los puntos más complejos fue la codificación de texto que por naturaleza es más costoso computacionalmente y es díficil obtener información valiosa si no se aplican los métodos correctos.\n",
    "\n",
    "Hablando ya de los modelos, sobre la clasficación se obtuvo un baseline cercano a los 0.25 con un DecisionTree, algo levemente mayor a la asignación aleatoria, por lo que se aplican técnicas de elección de hiperparámetros para poder mejorar el performance. En esta grilla, se ocuparon los modelos Regresión Logística, Random Forest, KNeighbors y XGBoost con cambios en la complejidad de los algoritmos como perturbaciones en la profundidad del análisis del texto. Esta búsqueda demora cerca de 40 minutos en ejecutarse y el rendimiento óptimo se lo lleva el XGBoost, pero que no es muy superior al baseline y dificilmente llega a los 0.3.\n",
    "\n",
    "Para el modelo de regresión el razonamiento fue similar. Se ocupó como baseline una regresión lineal que tenía como R2 base 0.53, algo bastante decente para ser el modelo tan simple. Nuevamente se ocupó GridSearch para buscar una mejor combinación entre los modelos Regresión Lineal, RandomForest y XGBoost, siendo el RandomForest el de mejor performance. Sin embargo, dada la complejidad, la pérdida parcial de interpretabilidad y tiempo en entrenar se opta por el segundo mejor modelo: Regresión Lineal. Notar que la diferencia entre ambos R2 es de 0.00051. La mejora producida por el GridSearch es muy menor a lo esperado y tiene un peak de 0.568, bastate cercano a los 0.53 del baseline.\n",
    "\n",
    "En base a todo lo anterior, es que se puede señalar que los resultados son buenos, pero no óptimos, dado que se puedieron crear muchísimas más variables en la etapa de preprocesamiento si se contara con el conocimiento en procesamiento de lenguaje. La gran cantidad de columnas de texto hizo dificil la creación de nuevos atributos, pero es posible que si se extrayera mayor información el performance de los modelos sea mejor.\n",
    "\n",
    "También, el modelo de clasificación, el modelo usualmente confundía a las clases positivas, dada su cercanía en puntaje. Para un mejor modelamiento se cree que reagrupar las variables de puntuación es lo más conveniente (por ej: Negativa, Mix, Positiva) dado que se pueden diferenciar mejor los comportamientos de los usuarios si es que no se segrega con las puntuaciones intermedias.\n",
    "\n",
    "A métodos generales, el proyecto ha sido una gran oportunidad de poner en práctica los distintos conocimientos en ciencia de datos que se han aprendido en cursos anteriores como en este curso práctico. En retrospectiva, uno de los elementos que personalmente ha sido más complejo es el procesamiento de texto, el cual es una rama completa de estudio en la ciencia de datos y que se espera poder profundizar en cursos posteriores, sin embargo, el curso en sí es bastante completo en los tópicos que se abarcan y lograr generan un aprendizaje rápido y continuo en los temás más esenciales del Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfebeb",
   "metadata": {
    "cell_id": "00027-1e362e1d-a776-4423-93d5-0f568476e4c1",
    "deepnote_cell_height": 84.10000610351562,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Anexo: Generación de Archivo Submit de la Competencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749de65d",
   "metadata": {
    "cell_id": "00028-0a64e7e8-1077-4868-8c96-db3d51323157",
    "deepnote_cell_height": 671.9000244140625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Para subir los resultados obtenidos a la pagina de CodaLab utilice la función `generateFiles` entregada mas abajo. Esto es debido a que usted deberá generar archivos que respeten extrictamente el formato de CodaLab, de lo contario los resultados no se veran reflejados en la pagina de la competencia.\n",
    "\n",
    "Para los resultados obtenidos en su modelo de clasificación y regresión, estos serán guardados en un archivo zip que contenga los archivos `predicctions_clf.txt` para la clasificación y `predicctions_rgr.clf` para la regresión. Los resultados, como se comento antes, deberan ser obtenidos en base al dataset `test.pickle` y en cada una de las lineas deberan presentar las predicciones realizadas.\n",
    "\n",
    "Ejemplos de archivos:\n",
    "\n",
    "- [ ] `predicctions_clf.txt`\n",
    "\n",
    "        Mostly Positive\n",
    "        Mostly Positive\n",
    "        Negative\n",
    "        Positive\n",
    "        Negative\n",
    "        Positive\n",
    "        ...\n",
    "\n",
    "- [ ] `predicctions_rgr.txt`\n",
    "\n",
    "        16103.58\n",
    "        16103.58\n",
    "        16041.89\n",
    "        9328.62\n",
    "        107976.03\n",
    "        194374.08\n",
    "        ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1016c881",
   "metadata": {
    "cell_id": "00029-55f95a4c-2d1f-4354-a690-049fea34bdac",
    "deepnote_cell_height": 620.13330078125,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166501,
    "source_hash": "b1cdf32f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "def generateFiles(data, clf_pipe, rgr_pipe):\n",
    "    \"\"\"Genera los archivos a subir en CodaLab\n",
    "\n",
    "    Input\n",
    "    predict_data: Dataframe con los datos de entrada a predecir\n",
    "    clf_pipe: pipeline del clf\n",
    "    rgr_pipe: pipeline del rgr\n",
    "\n",
    "    Ouput\n",
    "    archivo de txt\n",
    "    \"\"\"\n",
    "    y_pred_clf = le.inverse_transform(clf_pipe.predict(data))\n",
    "    y_pred_rgr = rgr_pipe.predict(data)\n",
    "    \n",
    "    with open('./predictions_clf.txt', 'w') as f:\n",
    "        for item in y_pred_clf:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    with open('./predictions_rgr.txt', 'w') as f:\n",
    "        for item in y_pred_rgr:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    with ZipFile('predictions.zip', 'w') as zipObj2:\n",
    "       zipObj2.write('predictions_rgr.txt')\n",
    "       zipObj2.write('predictions_clf.txt')\n",
    "\n",
    "    os.remove(\"predictions_rgr.txt\")\n",
    "    os.remove(\"predictions_clf.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89d97719",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val2 = data_val.drop(['target', 'credits', 'genres',\n",
    "               'original_language', 'production_companies',\n",
    "               'release_date', 'label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2bf1da6f",
   "metadata": {
    "cell_id": "b589f659ce2246919e3707e79420aff2",
    "deepnote_cell_height": 138,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ejecutar función para generar el archivo de predicciones.\n",
    "# perdict_data debe tener cargada los datos del text.pickle\n",
    "# mientras que clf_pipe y rgr_pipe, son los pipeline de \n",
    "# clasificación y regresión respectivamente.\n",
    "generateFiles(data_val2, grid_model_clf, pipe_reg_grid_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173dca5b",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "dbddc0f3-10b8-4160-bc27-ac993196164c",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
